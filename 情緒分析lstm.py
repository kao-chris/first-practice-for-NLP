# -*- coding: utf-8 -*-
"""情緒分析LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17AVSA5eXdIFojt3_Bw-zYEmbCeymTpdZ

安裝tensorflow
"""

!pip install tensorflow==1.15.4

"""安裝keras"""

!pip install keras==2.1.6

"""讀取雲端硬碟"""

from google.colab import drive
drive.mount('/content/drive')

"""匯入參數"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import keras
from keras.models import load_model
from IPython.display import clear_output
np.random.seed(10)
keras.__version__

"""匯入CSV檔"""

excel_file = pd.read_csv('/content/drive/MyDrive/csv情緒/semeval-2017-dev.csv',sep='\t')
model_train1=excel_file
excel_file = pd.read_csv('/content/drive/MyDrive/csv情緒/semeval-2013-dev.csv',sep='\t')
model_train2=excel_file
excel_file = pd.read_csv('/content/drive/MyDrive/csv情緒/semeval-2014-test.csv',sep='\t')
model_train3=excel_file
#excel_file = pd.read_csv('/content/drive/MyDrive/csv情緒/semeval-2017-train.csv',sep='\t')
#model_train4=excel_file
#excel_file = pd.read_csv('/content/drive/MyDrive/csv情緒/semeval-2013-test.csv',sep='\t')
#model_train5=excel_file

model_train=pd.concat([model_train1,model_train2,model_train3])#,model_train5

"""印出檔案"""

print(len(model_train))
model_train.head()

df = model_train

import csv
csvfile = open('semeval-2017-dev.csv' , 'r')
reader = csv.reader(csvfile)
# 以字典形式輸出，第一行為字典的鍵
reader = csv.DictReader(csvfile)
rows = [row for row in reader]
print(rows)

tweet_df = df[['label','text']]
tweet_df.head(10)

tweet_df = tweet_df[tweet_df['label'] != 'neutral']

model_train = tweet_df.label.factorize()

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tweet = tweet_df.text.values
tokenizer = Tokenizer(num_words=500)
tokenizer.fit_on_texts(tweet)

vocab_size = len(tokenizer.word_index) + 1

encoded_docs = tokenizer.texts_to_sequences(tweet)

padded_sequence = pad_sequences(encoded_docs, maxlen=200)

print(tokenizer.word_index)

print(tweet[0])
print(encoded_docs[0])

print(padded_sequence[0])

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM , Dense, Dropout,Flatten 
from tensorflow.keras.layers import SpatialDropout1D , Conv1D , MaxPooling1D , ZeroPadding2D,Activation
from tensorflow.keras.layers import Embedding , Bidirectional
embedding_size=200 #词向量维度
maxlen=200 #最大样本长度，不足进行Padding，超过进行截取
lstm_output_size=200 #LSTM层的输出维度
#训练参数
#batch_size=128
#epochs=20
kernel_size=5
filters=128
pool_size=4
embedding_vector_length = 32

model=Sequential()
model.add(Embedding(vocab_size, embedding_vector_length,     
                  input_length=200) )

model.add(Dropout(0.2))
model.add(Conv1D(filters,kernel_size,padding="valid",activation="sigmoid",strides=1))
model.add(Dropout(0.4))
model.add(MaxPooling1D(pool_size))
model.add(Bidirectional(LSTM(200,return_sequences=True),merge_mode='concat'))
model.add(Dropout(0.2))
#model.add(Dense(1,Activation("tanh")))
#model.add(Conv1D(filters,kernel_size,padding="valid",activation="sigmoid",strides=1))
#model.add(Dropout(0.35))
#model.add(MaxPooling1D(pool_size))
#model.add(Conv1D(filters,kernel_size,padding="valid",activation="sigmoid",strides=1))
#model.add(Dropout(0.4))
#model.add(MaxPooling1D(pool_size))

model.add(Activation("sigmoid"))
model.add(Flatten())
model.add(Dropout(0.3))

#model.add(Dropout(0.35))
#model.add(SpatialDropout1D(0.2))

#model.add(LSTM(30, dropout=0.2, recurrent_dropout=0.5))

#model.add(Dropout(0.2))

model.add(Dense(1, Activation("tanh")))
model.add(Dropout(0.35))
#model.add(Dense(1,Activation("softmax")))


model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])



print(model.summary())

history = model.fit(padded_sequence,model_train[0],
                      validation_split=0.35, epochs=5, batch_size=64)

from matplotlib import pyplot as plt

plt.plot(history.history['acc'], label='acc')
plt.plot(history.history['val_acc'], label='val_acc')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.legend()
plt.show()

test_word ="this is a happy things  "

tw = tokenizer.texts_to_sequences([test_word])
tw = pad_sequences(tw,maxlen=200)
prediction = int(model.predict(tw).round().item())


model_train[-1][prediction]